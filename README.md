fmperf repository: https://github.com/fmperf-project/fmperf

# Setup

Set your preferences in the [.env](.env) file. Key settings:
- Duration: Set `DURATION` for each iteration length.
- URL: Set `URL` to your model endpoint.
- Tokens list: Set `TOKENS_LIST` as comma-separated pairs (e.g., `32:32,64:64`).
- REQ_MIN start: Set `REQ_MIN_START` to the initial requests/min value.
- REQ_MIN increase: Set `REQ_MIN_INCREASE_MULTIPLIER` to control growth during stage 1.
- Stop threshold: Set `STOP_THRESHOLD` for the stage 2 termination criterion.

Notes:
- The values `MIN/MAX_INPUT/OUTPUT_TOKENS` are set per iteration from `TOKENS_LIST`; the [.env](.env) file is not modified during runs.
- `REQ_MIN` changes automatically per iteration based on `REQ_MIN_INCREASE_MULTIPLIER` (stage 1) and binary search (stage 2); configuration is read from [.env](.env) by [experiment_automation.py](experiment_automation.py).

# How to run

Once everything is set up, run experiment_automation.py. experiment_automation.slurm creates a SLURM job that executes the experiment in the background.

# Results

The results can be found in the folder /requests, under the name XXX_YYY, where XXX is the number of input tokens of the iteration and YYY is the number of output tokens of the iteration. Within these folders, you will find more folders with the timestamp of each iteration. Finally, here you will find "first_half.csv", "second_half.csv", "output.csv", "results.csv" and "results.json". "first_half.csv" and "second_half.csv" is a summary of the response time of tokens generated in the first and second halves of the experiment, and "output.csv" is the file from which these two are obtained. "results.json" is the standard output of fmperf, where you can find information per token generated. Finally, "results.csv" is a summary of the results obtained from the iteration, although a few of the attributes found in this file are deprecated and would only be of use in our HPC, the most relevant ones are "INPUT_TOKENS", "OUTPUT_TOKENS", "EVALUATION" and "REQ_MIN"; EVALUATION is True if the iteration is deemed sustainable.

# How it works

The experiment consists of a set of iterations. During an iteration, of a specified duration, a number of requests will be sent per minute. The number of requests depends on the value of REQ_MIN. 

The experiment consists of two stages:

## Find non-sustainable value
REQ_MIN starts at `REQ_MIN_START` and is increased by `REQ_MIN_INCREASE_MULTIPLIER` between iterations. At the end of each iteration, we use the code in [requests/evaluate.py](requests/evaluate.py) to determine if the iteration is sustainable or not. As long as the iterations performed are sustainable, we continue to increase REQ_MIN. As soon as one of them is not sustainable, this stage ends.

## Find MST
We set _m_ to the highest stable value for REQ_MIN and _M_ to the found unsustainable value of REQ_MIN. A binary search is performed, where REQ_MIN is set to the in-between value of _M_ and _m_ and we run an iteration. We evaluate the result, and update _m_ or _M_ accordingly, based on whether the value is deemed sustainable or not. This stage ends once _M - m_ is less than or equal to `STOP_THRESHOLD`.

## More information

Here you will find a few more pieces of information regarding how the experiment is handled:

### What is the prompt?
The prompt is generated by fmperf, by running the command `fmperf.loadgen.generate-input`, which is automatically run in the [experiment_automation.py](experiment_automation.py) file. The prompt takes tokens from [fmperf/data/ai.txt](fmperf/data/ai.txt). The number of tokens is determined per iteration by `MIN/MAX_INPUT_TOKENS`, derived from `TOKENS_LIST`.

### Why do I see empty folders with names such as 128_128?
These are placeholder folders where the results of the experiment are stored before being moved to their corresponding location within the /requests folder.