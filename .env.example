# benchmark target: {vllm, tgis}
TARGET=vllm

# minimum number of input tokens
MIN_INPUT_TOKENS=2048

# maximum number of inputs tokens
MAX_INPUT_TOKENS=2048

# minimum number of output tokens
MIN_OUTPUT_TOKENS=2048

# maximum number of output tokens
MAX_OUTPUT_TOKENS=2048

# fraction of greedy requests
FRAC_GREEDY=1.0

# number of input requests to generate (virtual users will sample from these)
SAMPLE_SIZE=1


# use code prompts (rather than normal text)
CODE=false

# Default requests directory
REQUESTS_DIR=requests

# requests file
REQUESTS_FILENAME=sample_requests.json

# results file
RESULTS_FILENAME=results.json

# filename for combined results
RESULTS_ALL_FILENAME=results_all.json

# number of virtual users
NUM_USERS=3

# number of requests per minute
REQ_MIN=2048

# number of virtual users
SWEEP_USERS=5

# experiment duration
DURATION=120s

# if a request fails, we will backoff for some time
BACKOFF=1s

# we allow some grace period for requests to finish when experiment ends
GRACE_PERIOD=5s

# URL for inference server endpoint
# for vLLM this should look like: $(IP_ADDRESS):8000
# and for TGIS this should look like $(IP_ADDRESS):8033
URL=gpu06:9000
