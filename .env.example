# benchmark target: {vllm, tgis}
TARGET=vllm

# fraction of greedy requests
FRAC_GREEDY=1.0

# number of input requests to generate (virtual users will sample from these)
SAMPLE_SIZE=1

# use code prompts (rather than normal text)
CODE=false

# Default requests directory
REQUESTS_DIR=requests

# requests file
REQUESTS_FILENAME=sample_requests.json

# results file
RESULTS_FILENAME=results.json

# filename for combined results
RESULTS_ALL_FILENAME=results_all.json

# number of virtual users
NUM_USERS=3

# number of virtual users
SWEEP_USERS=5

# if a request fails, we will backoff for some time
BACKOFF=1s

# we allow some grace period for requests to finish when experiment ends
GRACE_PERIOD=5s

# URL for inference server endpoint
# for vLLM this should look like: $(IP_ADDRESS):8000
# and for TGIS this should look like $(IP_ADDRESS):8033
URL=gpu06:9000

# --- Experiment automation configuration ---
# List of input/output token pairs to test (format: a:b,a:b,...)
TOKENS_LIST=16:16,32:32,64:64,128:128,256:256

# Starting value for REQ_MIN used by experiment automation
REQ_MIN_START=2048,2048,1024,512,512

# Multiplicative factor to increase REQ_MIN on successful evaluation in stage 1
REQ_MIN_INCREASE_MULTIPLIER=2.0

# Threshold for stop condition in stage 2 (used in M - m <= STOP_THRESHOLD)
STOP_THRESHOLD=10

# experiment iteration duration
DURATION=1800s

# Buffer second filter
FILTER_BUFFER=300

# --- variables that change automatically ---
# minimum number of input tokens
MIN_INPUT_TOKENS=2048

# maximum number of inputs tokens
MAX_INPUT_TOKENS=2048

# minimum number of output tokens
MIN_OUTPUT_TOKENS=2048

# maximum number of output tokens
MAX_OUTPUT_TOKENS=2048

# number of requests per minute
REQ_MIN=2048